{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.48 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\prayt\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "import json\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import demoji\n",
    "\n",
    "import sys\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import JSON\n",
    "\n",
    "demoji.download_codes()\n",
    "access_token = os.environ['ACCESS_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save or load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = {'raw_data':ads_df}\n",
    "# with open('raw_data.pickle', 'wb') as f:\n",
    "#     pickle.dump(raw_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# experiment_pickle = {'experiment_data':experimental_search}\n",
    "# with open('experiment_pickle.pickle', 'wb') as f:\n",
    "#     pickle.dump(experiment_pickle, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# joined_congress_candidates = {'joined_congress_candidates':experiment_2}\n",
    "# with open('joined_congress_candidates.pickle', 'wb') as f:\n",
    "#     pickle.dump(joined_congress_candidates, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "## emoji removed labeled leans for test/train data merged to df appended, urls removed, \n",
    "# emoji_removed = {'emoji_removed':experiment_2,\n",
    "#                 'entity_labels':entity_leans}\n",
    "# with open('emoji_removed.pickle', 'wb') as f:\n",
    "#     pickle.dump(emoji_removed, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "## special_char_removed - this is post emoji removal\n",
    "# special_char_removed = {'special_char_removed':experiment_2,\n",
    "#                 'entity_labels':entity_leans}\n",
    "# with open('special_char_removed.pickle', 'wb') as f:\n",
    "#     pickle.dump(special_char_removed, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('special_char_removed.pickle', 'rb') as f:\n",
    "#     a_pickle = pickle.load(f)\n",
    "# experiment_2 = a_pickle['special_char_removed']\n",
    "# # congress = a_pickle['congress']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is helpful when labeling entity leans\n",
    "\n",
    "def lookup_funder(funder):\n",
    "    return experiment_2.loc[experiment_2['funding_entity']==funder]['ad_creative_body']\n",
    "    \n",
    "#lookup_funder(\"The Family Protection Association\")#[56383]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_ads(srch_term, num_scrapes=8, search_term_lean=None):\n",
    "    \n",
    "    url='https://graph.facebook.com/v4.0/ads_archive/'\n",
    "\n",
    "    response = requests.get(url,params={\"access_token\":access_token,\n",
    "                        \"ad_type\":\"POLITICAL_AND_ISSUE_ADS\",\n",
    "                        \"search_terms\":srch_term,\n",
    "                        \"ad_reached_countries\":\"['US']\",\n",
    "                        \"fields\":\"ad_creative_link_description,ad_creative_link_title,funding_entity,ad_creative_body,ad_snapshot_url\",\n",
    "                        \"limit\":200\n",
    "                        })\n",
    "        \n",
    "    if len(response.json())!=2:\n",
    "        print('200 results or less')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    ad_data = pd.DataFrame(response.json()['data'])\n",
    "    \n",
    "    for x in list(range(0,num_scrapes)):\n",
    "        response = requests.get(response.json()['paging']['next'])\n",
    "        #print(x)\n",
    "        #print(response.json().keys())\n",
    "        if len(response.json())!=2:\n",
    "            print('bingo', x)\n",
    "            ad_data['srch_term'] = srch_term\n",
    "            return ad_data\n",
    "        else:\n",
    "            ad_data = ad_data.append(response.json()['data'], ignore_index=True)\n",
    "    \n",
    "    ad_data['srch_term'] = srch_term\n",
    "    \n",
    "#     if search_term_lean != None:\n",
    "#         ad_data['srch_term_lean'] = srch_term_lean\n",
    "    \n",
    "    return ad_data\n",
    "\n",
    "\n",
    "def scrape_topics(topic_list, num_scrapes=8):\n",
    "    #calls scrape ads for multiple search terms\n",
    "    #topic list must be list of strings\n",
    "    ads_df=pd.DataFrame()\n",
    "    for topic in topic_list:\n",
    "        try:\n",
    "            a = scrape_ads(topic, num_scrapes)\n",
    "            ads_df = ads_df.append(a, ignore_index=True)\n",
    "        except: \n",
    "            print('error in topic: ' + topic)\n",
    "    return ads_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URL Function\n",
    "def remove_url(text):\n",
    "    # findall() has been used  \n",
    "    # with valid conditions for urls in string \n",
    "    text = re.sub('HTTP[S]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))\\S+','', str(text))\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))\\S+','', str(text))\n",
    "    text = re.sub('www.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))\\S+','', str(text))\n",
    "    text = re.sub('WWW.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))\\S+','', str(text))\n",
    "    text = re.sub('bit.ly\\S+','', text)  \n",
    "    return text \n",
    "\n",
    "def remove_links(df, column):\n",
    "    #column must be string\n",
    "    df[column] = df[column].map(lambda x: remove_url(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    text = str(text)\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "#     # Optionally, remove stop words\n",
    "#     if remove_stopwords:\n",
    "#         stops = set(stopwords.words(\"english\"))\n",
    "#         text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    #text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "#     # Optionally, shorten words to their stems\n",
    "#     if stem_words:\n",
    "#         text = text.split()\n",
    "#         stemmer = SnowballStemmer('english')\n",
    "#         stemmed_words = [stemmer.stem(word) for word in text]\n",
    "#         text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "def clean_all_docs(df, column):\n",
    "    print('got this far')\n",
    "    df[column] = df[column].map(lambda x: text_to_wordlist(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emojis and replace with descriptor for one string\n",
    "\n",
    "def replace_an_emoji(text):\n",
    "    #depencies - install and import demoji\n",
    "    temp_dict = demoji.findall(text)\n",
    "    for emoji in temp_dict.keys():\n",
    "        text = text.replace(emoji, temp_dict[emoji])\n",
    "    return text\n",
    "\n",
    "# Remove emojis and replace with descriptor for dataframe column\n",
    "def replace_all_emoji(df, column):\n",
    "    #column must be string\n",
    "    df[column] = df[column].map(lambda x: replace_an_emoji(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call API for all keywords below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "srch_topics = ['socialist','trade war', 'wall', 'medicare', \n",
    "                'invasion','veterans', 'mexico', 'me too', 'gay rights', \n",
    "                'black lives matter', 'white nationalism', 'terrorism', 'corrupt',\n",
    "                'muslim brotherhood', 'global warming', 'climate change', 'trade', 'trade war',\n",
    "                'china', 'farmers','religious liberty', 'nato','russia', 'elections','hillary',\n",
    "                'benghazi', 'space force', 'pro-life', 'abortion','pro-choice', 'RBG', 'kavanaugh',\n",
    "                'feel the bern', 'middle east', 'military spending', 'president',\n",
    "                'obama', 'mitch mcconnell', 'AOC', 'squad', 'court','blue lives matter',\n",
    "                'schumer', 'kamala harris', 'biden', 'GOP', 'democratic','conservative',\n",
    "                'liberal', 'DNC', 'arpaio', 'gun violence', '2nd ammendment','maga',\n",
    "                'trump', 'kaepernick','feminism','science','antifa','tea party', 'dc statehood',\n",
    "                'dc statehood','diversity', 'vulnerable','sexual assault', 'safe space', 'elites',\n",
    "                '99%', 'safety net', 'welfare', 'trickle down', 'student loans', 'social security',\n",
    "                'obamacare', 'affordable care act', 'opioid crisis','kim jong un', 'putin', 'flag',\n",
    "                'tlaib', 'ilhan omar','israel','lincoln monument', '4th of july', 'baltimore', 'charlottesville',\n",
    "                'pelosi','hannity', 'maddow','warren','native american','pocahantas','standing rock','pollution',\n",
    "                'devin nunes', 'john mccain', 'epa', 'cfpb', 'wall street','elijah cummings','conspiracy',\n",
    "                'hate speech', 'amazon', 'big tech', 'zuckerberg','campaign finance', 'citizens united', 'maxine waters',\n",
    "                'enemy of the people', 'journalism','democratic debate','debt ceiling', 'sherrod brown', 'ted cruz',\n",
    "                'beto','iran','obstruction of justice','endangered species','environment','nuclear weapons',\n",
    "                'brexit','racism','bias','coal', 'west', 'sovereign citizen', 'cliven bundy']\n",
    "\n",
    "scraped_ads =  scrape_topics(srch_topics, num_scrapes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelim data cleaning to remove duplicates - each NLP modeling technique requires different data cleaning steps so more cleaning will occur later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_ads = remove_links(scraped_ads.ad_creative_body)\n",
    "scraped_ads = scraped_ads.drop_duplicates(subset=['ad_creative_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save scraped ads in pickle for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_ads = {'scraped_ads':scraped_ads}\n",
    "with open('scraped_ads.pickle', 'wb') as f:\n",
    "    pickle.dump(scraped_ads, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort funding entities by the number of unique ads in the dataset and save to CSV for identification of left wing and right wing funding entities.  From here left/right wing entities need to be labeled by hand and loaded back in the section under 'Create Training data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = pd.DataFrame(scraped_ads.funding_entity.value_counts()).reset_index()\n",
    "csv_df.to_csv(r'~/projects_folder/side_projects_B/ad_recommender/label_entities_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training data labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create left wing/right wing ad labels by merging labeled funding entities csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scraped_ads.pickle', 'rb') as f:\n",
    "    a_pickle = pickle.load(f)\n",
    "scraped_ads = a_pickle['scraped_ads']\n",
    "entity_leans = pd.read_csv('label_entities_file.csv', encoding = \"ISO-8859-1\");\n",
    "scraped_ads = scraped_ads.merge(entity_leans, how='left', on='funding_entity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset and clean labeled train/test data for LSTM NN, export to csv, aws_model_file_x picks it up from here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = scraped_ads.loc[(scraped_ads['lean']=='d')|(scraped_ads['lean']=='r'), ['ad_creative_body','lean']].reset_index().drop(columns='index')\n",
    "labeled_data = remove_links(labeled_data, 'ad_creative_body')\n",
    "labeled_data = labeled_data.drop_duplicates(subset='ad_creative_body')\n",
    "labeled_data = replace_all_emoji(labeled_data, 'ad_creative_body')\n",
    "labeled_data = clean_all_docs(labeled_data, 'ad_creative_body')\n",
    "labeled_data.loc[labeled_data['lean']=='d','lean'] = 1\n",
    "labeled_data.loc[labeled_data['lean']=='r','lean'] = 0\n",
    "labeled_data.to_csv('C:/Users/prayt/projects_folder/side_projects_B/ad_recommender/labeled_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
